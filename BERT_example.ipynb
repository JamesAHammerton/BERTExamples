{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BERT (tensorflow version) for sentiment classification\n",
    "Derived from: [A visual guide to using BERT for the first time.](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)\n",
    "\n",
    "Uses the [Hugging Face](https://huggingface.co/) [transformers library](https://github.com/huggingface/transformers): \n",
    "\n",
    "## First we load and inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  a stirring , funny and finally transporting re...  1\n",
       "1  apparently reassembled from the cutting room f...  0\n",
       "2  they presume their audience wo n't sit still f...  0\n",
       "3  this is a visually stunning rumination on love...  1\n",
       "4  jonathan parker 's bartleby should have been t...  1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The data consists of text, label pairs where the label is 1 (positive sentiment) or 0 (negative sentiment)\n",
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hold of the Bert tokeniser and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c236feabad52473ca1a4202e80732951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the data and then obtain Bert embeddings for each sentence. \n",
    "\n",
    "Here we process the tokenized sentences in batches - this was due to running out of memory on my 8GB iMac when\n",
    "running the model on all the sentences at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(df[0].tolist(), padding=True, truncation=True, return_tensors='tf')\n",
    "\n",
    "def get_batches(tokenized, batch_size=1000):\n",
    "    # We expected tokenized to be a dictionary containing two entries:\n",
    "    #     inputs_ids\n",
    "    #     attention_mask\n",
    "    # both are arrays of the same length.\n",
    "    length = len(tokenized['input_ids'])\n",
    "    for b in range(0, length, batch_size):\n",
    "        batch = {}\n",
    "        for k, v in tokenized.items():\n",
    "            batch[k] = v[b:b + batch_size]\n",
    "        yield batch\n",
    "\n",
    "def get_features_for_batch(batch):\n",
    "    outputs = model(batch)\n",
    "    return outputs[0][:,0,:].numpy()\n",
    "\n",
    "features = None\n",
    "\n",
    "start = time.time()\n",
    "num = 0\n",
    "for batch in get_batches(tokenized):\n",
    "    batch_features = get_features_for_batch(batch)\n",
    "    if features is None:\n",
    "        features = batch_features\n",
    "    else:\n",
    "        features = np.append(features, batch_features, 0)\n",
    "    print(f'Processed batch: {num}')\n",
    "    num += 1\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f'Generating {len(df)} embeddings took: {elapsed:.4f} seconds. This is {elapsed / len(df):.4f} seconds per embedding.')\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the labels back in and create the train / test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[1]\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a logistic regression classifier and report the accuracy on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(max_iter=1000)\n",
    "lr_clf.fit(train_features, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.57%, computed over 1730 test items.\n",
      "Evaluation took 0.0085 seconds, or 0.000005 s per test item.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(f'Accuracy: {100 * lr_clf.score(test_features, test_labels):.2f}%, computed over {len(test_features)} test items.')\n",
    "elapsed = time.time() - start\n",
    "print(f'Evaluation took {elapsed:.4f} seconds, or {elapsed/len(test_features):.6f} s per test item.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertExamples",
   "language": "python",
   "name": "bertexamples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
